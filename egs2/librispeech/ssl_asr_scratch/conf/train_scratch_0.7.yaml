grad_clip: 5.0
batch_type: numel
batch_bins: 20000000
accum_grad: 1
max_epoch: 200
patience: none
# The initialization method for model parameters
init: xavier_uniform
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10

input_size: 768
encoder: hubert_pretrain
encoder_conf:
    output_size: 768
    linear_units: 3072
    attention_heads: 8
    num_blocks: 12
    dropout_rate: 0.1
    attention_dropout_rate: 0.0
    dropout_input: 0.1
    dropout_features: 0.1
    # TODO(jzmo): add comments here
    skip_masked: false
    skip_nomask: false
    mask_prob: 0.80
    extractor_mode: default
    conv_feature_layers: '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'
    final_dim: 256
    encoder_layerdrop: 0.05
    feature_grad_mult: 0.1
    untie_final_proj: true
    label_rate: 50
    sample_rate: 16000

decoder: transformer
decoder_conf:
    attention_heads: 8
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.1
    src_attention_dropout_rate: 0.1

model_conf:
    hubert_weight: 0.7
    lsm_weight: 0.1
    length_normalized_loss: false
    pred_masked_weight: 1.0
    pred_nomask_weight: 0.0
    loss_weights: 10.0

optim: adam
optim_conf:
    lr: 0.0002
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 25000

unused_parameters: true
frontend: null
normalize: null
specaug: null
